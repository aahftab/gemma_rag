{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aahftab/gemma_rag/blob/main/gemma_rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DrBoa_Urw9Vx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Note: `userdata.get` is a Colab API. If you're not using Colab, set the env\n",
        "# vars as appropriate for your system.\n",
        "os.environ[\"KAGGLE_USERNAME\"] = userdata.get('KAGGLE_USERNAME')\n",
        "os.environ[\"KAGGLE_KEY\"] = userdata.get('KAGGLE_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UcGLzDeQ8NwN",
        "outputId": "ae50395c-bba1-41d4-e01e-3742596560be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m465.3/465.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m950.8/950.8 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.0.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Install Keras 3 last. See https://keras.io/getting_started/ for more details.\n",
        "!pip install -q -U keras-nlp\n",
        "!pip install -q -U keras>=3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ww83zI9ToPso"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "import keras_nlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rS7ryTs5wjf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"KERAS_BACKEND\"] = \"jax\"  # Or \"tensorflow\" or \"torch\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yygIK9DEIldp",
        "outputId": "82e17ae7-9fcf-454f-f349-d3f75340f264",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Attaching 'config.json' from model 'keras/gemma/keras/gemma_instruct_2b_en/2' to your Colab notebook...\n",
            "Attaching 'config.json' from model 'keras/gemma/keras/gemma_instruct_2b_en/2' to your Colab notebook...\n",
            "Attaching 'model.weights.h5' from model 'keras/gemma/keras/gemma_instruct_2b_en/2' to your Colab notebook...\n",
            "Attaching 'tokenizer.json' from model 'keras/gemma/keras/gemma_instruct_2b_en/2' to your Colab notebook...\n",
            "Attaching 'assets/tokenizer/vocabulary.spm' from model 'keras/gemma/keras/gemma_instruct_2b_en/2' to your Colab notebook...\n"
          ]
        }
      ],
      "source": [
        "gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma_instruct_2b_en\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5nEbTdApL7W"
      },
      "outputs": [],
      "source": [
        "gemma_lm.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aae5GHrdpj2_",
        "outputId": "b22bd26f-c444-4df5-c0e5-857f89a3e178",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"What is the meaning of life?\\n\\nThe question of the meaning of life has perplexed humanity for millennia, with no single, universally accepted answer. However, throughout history, various perspectives and beliefs have emerged, offering insights into the purpose and significance of our existence.\\n\\n**Philosophical Perspectives:**\\n\\n* **Existentialism:** Existentialists emphasize the individual's freedom and responsibility in creating their own meaning in a meaningless universe.\\n* **Nihilism:** Nihilists believe that life is ultimately pointless and without purpose, and that existence is devoid of meaning.\\n* **Stoicism:** Stoics focus on accepting what is beyond our control and finding meaning in what we can control, such as our relationships and personal growth.\\n* **Humanism:** Humanists prioritize human values and purpose, seeking meaning in creativity, compassion, and contributing to society.\\n\\n**Religious Perspectives:**\\n\\n* **Christianity:** Christians believe that life has a divine purpose, a plan that includes both suffering and redemption.\\n* **Islam:** Muslims emphasize the pursuit of knowledge, good character, and a life of service to God.\\n* **Judaism:** Jews hold the belief that life is sacred and that the ultimate purpose is to fulfill God's will through serving the Jewish\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "gemma_lm.generate(\"What is the meaning of life?\", max_length=256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEyTnnNGvgGG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "outputId": "9379da15-9737-4abb-cde2-0a8dddd62880"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'What is llm inference?\\n\\nLLM inference is a technique used in natural language processing (NLP) to improve the performance of large language models (LLMs) by leveraging auxiliary information from other sources. This information can include human-annotated data, external knowledge bases, or other LLMs.\\n\\n**How it works:**\\n\\n1. **Auxiliary information is extracted:** LLMs are trained on a massive dataset and learn representations of the text.\\n2. **Auxiliary information is used for inference:** When a new piece of text is presented, the LLM uses the auxiliary information to generate a probability distribution over the possible next words.\\n3. **The most likely next words are predicted:** The words with the highest probabilities are selected as the predicted next words.\\n\\n**Benefits of LLM inference:**\\n\\n* **Improved accuracy:** LLMs can achieve higher accuracy on tasks such as language translation, text summarization, and question answering by leveraging auxiliary information.\\n* **Enhanced robustness:** LLMs are less susceptible to biases and errors in the training data.\\n* **Increased efficiency:** LLMs can be trained more efficiently by using auxiliary information.\\n\\n**Examples of LLM inference:**\\n\\n* **Text generation:** Using external knowledge to generate more diverse and coherent text.\\n* **Sentiment analysis:** Using sentiment indicators from external sources to determine the emotional tone of a text.\\n* **Named entity recognition:** Using external knowledge to identify and classify named entities in a text.\\n\\n**Applications of LLM inference:**\\n\\n* **Chatbots:** LLMs can be trained with inference to provide more natural and informative responses.\\n* **Machine translation:** LLMs can be used to translate text between multiple languages by leveraging auxiliary information from translation dictionaries.\\n* **Text summarization:** LLMs can be used to generate concise summaries of long texts by focusing on the most important information.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "gemma_lm.generate(\"What is llm inference\", max_length=512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lsp2DyvrzP-N"
      },
      "outputs": [],
      "source": [
        "!pip install chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVWIRPegzWSD",
        "outputId": "99d12b1b-3922-4866-bbdf-ed8177db368f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34441\n"
          ]
        }
      ],
      "source": [
        "import chromadb\n",
        "import csv\n",
        "\n",
        "db = chromadb.Client()\n",
        "\n",
        "collection = db.get_or_create_collection(name=\"hadith\")\n",
        "\n",
        "with open(\"data.csv\", 'r') as file:\n",
        "  csv_reader = csv.DictReader(file)\n",
        "  data = [row for row in csv_reader]\n",
        "  id = [row[\"id\"] for row in data]\n",
        "  hadiths = [row[\"text\"] for row in data]\n",
        "\n",
        "print(len(hadiths))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g03wrFlfG47G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0,1000,1000):\n",
        "  collection.add(\n",
        "    documents=hadiths[i:i+1000],\n",
        "    ids=id[i:i+1000]\n",
        "  )\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29UL8lYzEtO-",
        "outputId": "ce6586d3-0f8a-4390-9e8e-3fa633e7b383"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/root/.cache/chroma/onnx_models/all-MiniLM-L6-v2/onnx.tar.gz: 100%|██████████| 79.3M/79.3M [00:01<00:00, 56.9MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "PtrZ-GNq61Pf",
        "outputId": "254145fc-17f4-4836-998a-d7ee4453ee8e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Answer query and context given below \\n Query: What are the duties of a man in Islam \\n Context: \\n Narrated Ibn \\'Umar:                          Allah\\'s Apostle said: Islam is based on (the following) five      (principles):     1. To testify that none has the right to be worshipped but Allah and      Muhammad is Allah\\'s Apostle.     2. To offer the (compulsory congregational) prayers dutifully and      perfectly.     3. To pay Zakat (i.e. obligatory charity) .     4. To perform Hajj. (i.e. Pilgrimage to Mecca)     5. To observe fast during the month of Ramadan.\\n Narrated Anas bin Malik:                     The Prophet said, \"Facilitate things to people (concerning religious matters), and do not make it hard for them and give them good tidings and do not make them run away (from Islam).\\n \\n\\nWhat are the duties of a man in Islam?\\n\\nAccording to the context, the duties of a man in Islam include testifying that none has the right to be worshipped but Allah and Muhammad is Allah\\'s Apostle, offering the compulsory congregational prayers dutifully and perfectly, paying Zakat, performing Hajj, and observing fast during the month of Ramadan.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "query = \"What are the duties of a man in Islam\"\n",
        "results = collection.query(\n",
        "    query_texts=[query],\n",
        "    n_results=2\n",
        ")\n",
        "result = ''\n",
        "for i in results['documents'][0]:\n",
        "  result += i + '\\n'\n",
        "\n",
        "gemma_lm.generate(f\"Answer query and context given below \\n Query: {query} \\n Context: \\n {result}\", max_length=512)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "google": {
      "image_path": "/site-assets/images/marketing/gemma.png",
      "keywords": [
        "examples",
        "gemma",
        "python",
        "quickstart",
        "text"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}